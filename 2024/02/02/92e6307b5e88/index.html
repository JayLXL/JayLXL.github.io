<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>信产投学习记录 | Jay and Erine's Blog</title><meta name="author" content="Jay and Erine"><meta name="copyright" content="Jay and Erine"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="FFF6E2"><meta name="description" content="环境 IDC :Internet Data Center 即互联网数据中心 华为私有云，主要用于提供基础环境 主要业务：图像识别应用，通过监控摄像头处理图像，实现市政管理监控，应用了商汤的算法模型，同时也可以自己训练  服务器信息[root@bms-2d36 ~]# uname -aLinux bms-2d36 4.19.90-23.8.v2101.ky10.aarch64 #1 SMP Mon">
<meta property="og:type" content="article">
<meta property="og:title" content="信产投学习记录">
<meta property="og:url" content="https://jayerine.top/2024/02/02/92e6307b5e88/index.html">
<meta property="og:site_name" content="Jay and Erine&#39;s Blog">
<meta property="og:description" content="环境 IDC :Internet Data Center 即互联网数据中心 华为私有云，主要用于提供基础环境 主要业务：图像识别应用，通过监控摄像头处理图像，实现市政管理监控，应用了商汤的算法模型，同时也可以自己训练  服务器信息[root@bms-2d36 ~]# uname -aLinux bms-2d36 4.19.90-23.8.v2101.ky10.aarch64 #1 SMP Mon">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s1.ax1x.com/2023/03/02/ppFkiKx.jpg">
<meta property="article:published_time" content="2024-02-02T06:30:37.425Z">
<meta property="article:modified_time" content="2024-02-13T02:48:32.390Z">
<meta property="article:author" content="Jay and Erine">
<meta property="article:tag" content="华为">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s1.ax1x.com/2023/03/02/ppFkiKx.jpg"><link rel="shortcut icon" href="https://s1.ax1x.com/2023/03/01/ppieNl9.png"><link rel="canonical" href="https://jayerine.top/2024/02/02/92e6307b5e88/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/all","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '信产投学习记录',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-13 10:48:32'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/fish.css"><script src="/js/sakura.js"></script><link rel="stylesheet" href="/css/footer_trans.css"><link rel="stylesheet" href="/css/scrollbar.css"><link rel="stylesheet" href="/css/cursor.css"><link rel="stylesheet" href="/css/mathjax_display.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Jay and Erine's Blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s1.ax1x.com/2023/02/27/ppCAPEQ.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Jay and Erine's Blog"><span class="site-name">Jay and Erine's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">信产投学习记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-02-02T06:30:37.425Z" title="Created 2024-02-02 14:30:37">2024-02-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-02-13T02:48:32.390Z" title="Updated 2024-02-13 10:48:32">2024-02-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="信产投学习记录"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul>
<li>IDC :Internet Data Center 即互联网数据中心</li>
<li>华为私有云，主要用于提供基础环境</li>
<li>主要业务：图像识别应用，通过监控摄像头处理图像，实现市政管理监控，应用了商汤的算法模型，同时也可以自己训练</li>
</ul>
<h3 id="服务器信息"><a href="#服务器信息" class="headerlink" title="服务器信息"></a>服务器信息</h3><p>[root@bms-2d36 ~]# uname -a<br>Linux bms-2d36 4.19.90-23.8.v2101.ky10.aarch64 #1 SMP Mon May 17 17:07:38 CST 2021 aarch64 aarch64 aarch64 GNU/Linux</p>
<ul>
<li>运行的是 Linux 内核版本为 4.19.90-23.8.v2101.ky10.aarch64。</li>
<li>aarch64架构</li>
</ul>
<p>aarch64架构是ARM（Advanced RISC Machine）架构中的一个64位版本。ARM架构最初是为嵌入式系统设计的，但现在也被广泛用于移动设备和一些服务器和超级计算机中。aarch64架构是ARMv8指令集架构的一个子集，提供了对64位处理器的支持。许多移动设备、嵌入式系统以及一些服务器都采用了aarch64架构，特别是在需要低功耗和高性能的场景下。</p>
<p>服务器使用的是atlas800平台，搭载了比较新的8块ascend910B npu，每块npu有32GB内存。</p>
<p>Atlas 300I 推理卡（型号：3000/3010）基于昇腾处理器，提供超强AI推理性能，支持80路高清视频实时分析，可广泛应用于智慧城市、智慧交通、智慧金融等场景。支持JPEG和视频硬件编解码，提升图片和视频类应用性能，例如支持H.264/H.265 Decoder硬件解码，80路1080P 25FPS，YUV420。</p>
<p>DVR代表数字视频录像机（Digital Video Recorder），而NVR代表网络视频录像机（Network Video Recorder）。</p>
<p>最开始以为服务器的芯片是ascend310，是推理卡，无法进行训练任务。后来经过查实发现搭载的是ascend910B，可以进行训练或推理（训练卡可能需要更多的显存，用于保存训练时的中间状态）。</p>
<p>功耗：实际在推理应用中，观察到单张npu的功耗在200W左右，闲置时的功率在60W附近。</p>
<p>cpu info<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@bms-2d36 ~]# lscpu</span><br><span class="line">Architecture:                    aarch64</span><br><span class="line">CPU op-mode(s):                  64-bit</span><br><span class="line">Byte Order:                      Little Endian</span><br><span class="line">CPU(s):                          192</span><br><span class="line">On-line CPU(s) list:             0-191</span><br><span class="line">Thread(s) per core:              1</span><br><span class="line">Core(s) per socket:              48</span><br><span class="line">Socket(s):                       4</span><br><span class="line">NUMA node(s):                    8</span><br><span class="line">Vendor ID:                       HiSilicon</span><br><span class="line">Model:                           0</span><br><span class="line">Model name:                      Kunpeng-920</span><br><span class="line">Stepping:                        0x1</span><br><span class="line">CPU max MHz:                     2600.0000</span><br><span class="line">CPU min MHz:                     200.0000</span><br><span class="line">BogoMIPS:                        200.00</span><br><span class="line">L1d cache:                       12 MiB</span><br><span class="line">L1i cache:                       12 MiB</span><br><span class="line">L2 cache:                        96 MiB</span><br><span class="line">L3 cache:                        192 MiB</span><br><span class="line">NUMA node0 CPU(s):               0-23</span><br><span class="line">NUMA node1 CPU(s):               24-47</span><br><span class="line">NUMA node2 CPU(s):               48-71</span><br><span class="line">NUMA node3 CPU(s):               72-95</span><br><span class="line">NUMA node4 CPU(s):               96-119</span><br><span class="line">NUMA node5 CPU(s):               120-143</span><br><span class="line">NUMA node6 CPU(s):               144-167</span><br><span class="line">NUMA node7 CPU(s):               168-191</span><br><span class="line">Vulnerability Itlb multihit:     Not affected</span><br><span class="line">Vulnerability L1tf:              Not affected</span><br><span class="line">Vulnerability Mds:               Not affected</span><br><span class="line">Vulnerability Meltdown:          Not affected</span><br><span class="line">Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl</span><br><span class="line">Vulnerability Spectre v1:        Mitigation; __user pointer sanitization</span><br><span class="line">Vulnerability Spectre v2:        Not affected</span><br><span class="line">Vulnerability Srbds:             Not affected</span><br><span class="line">Vulnerability Tsx async abort:   Not affected</span><br><span class="line">Flags:                           fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jsc</span><br><span class="line">                                 vt fcma dcpop asimddp asimdfhm ssbs</span><br></pre></td></tr></table></figure></p>
<h3 id="Linux服务器命令"><a href="#Linux服务器命令" class="headerlink" title="Linux服务器命令"></a>Linux服务器命令</h3><p>使用scp向服务器传输文件或从服务器下载文件，加入参数-r可以传输文件夹<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp Ascend-cann-toolkit_7.0.0_linux-aarch64.run root@172.27.84.235:/home/</span><br></pre></td></tr></table></figure><br>通过端口转发连接服务器<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -L 7777:localhost:7860 root@172.27.84.235</span><br></pre></td></tr></table></figure><br>端口转发需要修改sshd配置文件<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br><span class="line">PermitTunnel yes</span><br><span class="line">Forwarding yes</span><br><span class="line">systemctl restart sshd</span><br></pre></td></tr></table></figure><br>查看端口占用进程<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">netstat -tunlp |grep 7860</span><br></pre></td></tr></table></figure><br>export 命令配置临时运行环境<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export LD_PRELOAD=/usr/lib64/libgomp.so.1:$LD_PRELOAD</span><br></pre></td></tr></table></figure><br>配置终端默认环境<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><br>查看指定文件位置<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo find / -name file-name</span><br></pre></td></tr></table></figure><br>vim使用</p>
<ul>
<li>使用/进行查找</li>
<li>撤回：u</li>
</ul>
<p>查看进程<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ps -aux|grep jupyter</span><br></pre></td></tr></table></figure><br>显示包括操作系统信息在内的完整系统信息，如内核版本、主机名、处理器架构等。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">uname</span> -a</span><br></pre></td></tr></table></figure><br>显示包括操作系统名称、版本、ID等在内的系统发行版信息。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/os-release</span><br></pre></td></tr></table></figure><br>终止进程<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kill -9 PID</span><br></pre></td></tr></table></figure><br>下列命令可以开启后台进程，关闭ssh连接，不会终止进程。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup python -u  model_download.py  --repo_id THUDM/chatglm2-6b &gt;run_info.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>nohup指不断地运行，是no hang up的缩写，指不间断，不挂断。运行一个进程的时候，不想让其在你退出账号时关闭，即可用nohup。</li>
<li>-u：python的输出又缓冲，导致xxx.log并不能够马上看到输出。-u参数，使得python不启用缓冲。可以即时看到输出</li>
<li>xxx.log保存输出的信息，运行程序时会生成这个xxx文件，里面保存这输出的信息。</li>
<li>2&gt;&amp;1 将错误信息重定向到标准输出</li>
<li>&amp; 最后一个&amp;符号代表后台运行</li>
</ul>
<p>root路径的FileSystem爆满，无法下载东西，需要通过lvm命令进行硬盘扩容。</p>
<h3 id="常用Linux服务器目录"><a href="#常用Linux服务器目录" class="headerlink" title="常用Linux服务器目录"></a>常用Linux服务器目录</h3><ol>
<li><strong>bin</strong>：存放系统基本命令的可执行文件，一般是一些用户和管理员常用的命令。</li>
<li><strong>opt</strong>：存放可选软件包的安装目录，一些第三方软件通常安装在这个目录下。</li>
<li><strong>root</strong>：root用户的主目录。</li>
</ol>
<h3 id="atlas设备"><a href="#atlas设备" class="headerlink" title="atlas设备"></a>atlas设备</h3><p>在计算机领域中，NPU通常指的是神经网络处理器（Neural Processing Unit）。NPU是一种专门用于执行人工神经网络推断计算的硬件加速器，通常用于机器学习和人工智能应用中。</p>
<p>查询设备0所有芯片的统计信息。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npu-smi info -t usages -i 0</span><br></pre></td></tr></table></figure><br>查看服务器npu资源<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npu-smi -info</span><br></pre></td></tr></table></figure><br>NPU驱动和固件版本可通过以下命令查询。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npu-smi info -t board -i NPU ID</span><br></pre></td></tr></table></figure><br>查询设备0中编号为0的芯片的详细信息，可以发现芯片类型是910B。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npu-smi info -t board -i 0 -c 0</span><br><span class="line">        NPU ID                         : 0</span><br><span class="line">        Chip ID                        : 0</span><br><span class="line">        Chip Type                      : Ascend</span><br><span class="line">        Chip Name                      : 910B</span><br><span class="line">        Chip Version                   : V1</span><br><span class="line">        Board ID                       : 0x21</span><br><span class="line">        PCB ID                         : NA</span><br><span class="line">        BOM ID                         : 1</span><br><span class="line">        VDie ID                        : 185011DC 21B0230C 3858EA55 95A5090A 4D102003</span><br><span class="line">        NDie ID                        : 22D07194 21B0901C 3C58EA55 95A5090A CC102003</span><br><span class="line">        Chip Position ID               : 0</span><br><span class="line">        PCIe Bus Info                  : 0000:C1:00.0</span><br><span class="line">        Firmware Version               : 1.84.15.1.310</span><br></pre></td></tr></table></figure><br>照着gitee上的sample文档操作，发现可以生成om模型，但是在执行的时候，出现了报错。查询相关信息，发现是推理卡在推理的时候出错。猜测可能是cann的版本问题，机器上的cann是商业版6.0.1，MCU和NPU版本正常。按照教程将cann-toolkits更新到社区版8.0RC。</p>
<p>查看cann版本信息，安装成功。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">~# cat /opt/ascend-toolkit/latest/aarch64-linux/ascend_toolkit_install.info</span><br><span class="line">package_name=Ascend-cann-toolkit</span><br><span class="line">version=8.0.RC1.alpha001</span><br><span class="line">innerversion=V100R001C77B220SPC008</span><br><span class="line">compatible_version=[V100R001C80,V100R001C84],[V100R001C77,V100R001C79],[V100R001C29],[V100R001C11,V100R001C50]</span><br><span class="line">arch=aarch64</span><br><span class="line">os=linux</span><br><span class="line">path=/opt/ascend-toolkit/8.0.RC1.alpha001/aarch64-linux</span><br></pre></td></tr></table></figure><br>mindspore各个版本貌似要适配特定版本的cann。最新的社区版8.0还没有适配…<br>尝试安装7.0版本，报错固件版本不正确。但是却可以使用cann 6.0.1和cann 8.0….</p>
<p>去年12月，pytorch官方支持了Ascend设备，只需要安装一些依赖，然后将原来的.cuda换成.npu就能正常运行了。</p>
<p>运行时若需启动多张npu，需要导入环境变量：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export ASCEND_VISIBLE_DEVICES=0-7</span><br></pre></td></tr></table></figure><br>然后指定需要的npu，如npu:7。</p>
<h3 id="AIPP（AI-Preprocessing-Pipeline）："><a href="#AIPP（AI-Preprocessing-Pipeline）：" class="headerlink" title="AIPP（AI Preprocessing Pipeline）："></a>AIPP（AI Preprocessing Pipeline）：</h3><p>AIPP是一种用于深度学习推理的预处理管道，可用于对输入数据进行预处理以满足神经网络的输入要求。它可以执行诸如色彩空间转换、图像缩放、裁剪、对比度调整和格式转换等功能，以提高深度学习推理的效率和准确性。AIPP的存在可以帮助将图像数据转换为适合特定神经网络模型输入的格式，从而简化后续推理过程。</p>
<h4 id="DVPP（Data-Visuo-Processing-Pipeline）："><a href="#DVPP（Data-Visuo-Processing-Pipeline）：" class="headerlink" title="DVPP（Data Visuo-Processing Pipeline）："></a>DVPP（Data Visuo-Processing Pipeline）：</h4><p>DVPP是一种数据视觉处理管道，专门用于图像和视频数据的处理。它能够执行多种任务，包括图像解码、尺寸调整、颜色空间转换、图像格式转换和特定算法加速等。DVPP的存在可以帮助加速图像和视频数据的处理过程，提高处理效率和节省计算资源。</p>
<h3 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h3><h4 id="常用的conda环境管理命令"><a href="#常用的conda环境管理命令" class="headerlink" title="常用的conda环境管理命令"></a>常用的conda环境管理命令</h4><ol>
<li>创建一个新环境：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create --name myenv</span><br></pre></td></tr></table></figure></li>
<li>创建指定版本的Python环境：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create --name myenv python=3.8</span><br></pre></td></tr></table></figure></li>
<li>激活一个环境：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda activate myenv</span><br></pre></td></tr></table></figure></li>
<li>退出当前环境：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure></li>
<li>复制一个现有的环境：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create --name myclone --clone myenv</span><br></pre></td></tr></table></figure></li>
<li>删除一个环境：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda remove --name myenv --all</span><br></pre></td></tr></table></figure></li>
<li>查看所有环境<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure>
<h4 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h4>使用前需要激活conda 的 python环境<br>export PYENV_ROOT=”/root/anaconda3”</li>
</ol>
<p>linux下的conda create不会安装python，需要指定<br>conda install python=3.9</p>
<p>jupyter无法发现虚拟环境<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install ipkernel</span><br><span class="line">python -m ipykernel install --user --name npu_torch --display-name &quot;Python (torch)&quot;</span><br></pre></td></tr></table></figure></p>
<p>配置jupyter notebook自动换行不生效<br>查看配置文件，将多处配置文件同时修改确保生效</p>
<h3 id="mindspore"><a href="#mindspore" class="headerlink" title="mindspore"></a>mindspore</h3><p>看起来很像pytorch，配合cann-kit一起使用。</p>
<p>更新软件时，先更新固件，后更新驱动。</p>
<p>安装软件时，一定要注意官网上的流程，确保各种环境和版本都是适配的，否则很可能出现错误。</p>
<h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><p>简介：开发了名为torch_npu的Ascend Adapter for PyTorch插件，使昇腾NPU可以适配PyTorch框架，为使用PyTorch框架的开发者提供昇腾AI处理器的超强算力。</p>
<p><a target="_blank" rel="noopener" href="https://gitee.com/ascend/pytorch">ascend/pytorch项目链接</a></p>
<p>使用起来感觉非常方便，只需要import npu_torch，剩余都和cuda运行差不多。不过明显感觉适配度不高，有的模型运行很慢，或者运行出错。</p>
<p>使用python3.10时，会有如下报错，但是也能正常运行。更换python3.9之后正常运行。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SystemError: PY_SSIZE_T_CLEAN macro must be defined for &#x27;#&#x27; formats</span><br></pre></td></tr></table></figure></p>
<h2 id="onnx模型使用流程"><a href="#onnx模型使用流程" class="headerlink" title="onnx模型使用流程"></a>onnx模型使用流程</h2><p>要使用华为的Atlas 300进行推理（即使用NPU来运行深度学习模型进行预测），通常需要按照以下步骤进行：</p>
<ol>
<li><p><strong>准备模型</strong>：首先需要准备好要进行推理的深度学习模型。这包括模型文件（如TensorFlow模型、PyTorch模型等）以及模型的权重文件。</p>
</li>
<li><p><strong>转换模型</strong>：根据Atlas 300所支持的模型格式要求，有时需要将模型转换为适用于Atlas 300的格式。华为通常会提供相应的转换工具或者指导。</p>
</li>
<li><p><strong>使用MindSpore或TensorFlow等框架进行推理</strong>：一旦模型准备好，你可以使用支持Atlas 300的深度学习框架（比如华为的MindSpore）来进行推理。这通常会涉及使用框架提供的API加载模型，并将输入数据传递给模型进行预测。</p>
</li>
<li><p><strong>部署到Atlas 300硬件</strong>：将经过转换的模型部署到Atlas 300的硬件上进行推理，可以通过一些特定的接口和工具来实现。</p>
</li>
<li><p><strong>性能优化和调试</strong>：根据实际应用需求，可以通过调整模型参数、硬件配置、推理策略等手段对推理性能进行优化。</p>
</li>
</ol>
<p>DDK为用户提供基于NPU的数字开发者套件。DDK可以用于构建相关工程的编译环境。不同的发布包里集成了不同NPU形态的DDK。</p>
<p>在atlas平台上进行推理，需要将开源框架的网络模型，例如Caffe、TensorFlow等框架训练好的模型，通过OMG（Offline Model Generator：离线模型生成器）将其转换成昇腾AI处理器支持的离线模型，模型转换过程中可以实现算子调度的优化、权值数据重排、量化压缩、内存使用优化等，可以脱离设备完成模型的预处理。</p>
<h2 id="开发工具"><a href="#开发工具" class="headerlink" title="开发工具"></a>开发工具</h2><p>华为生态开发常用工具</p>
<ul>
<li>Atlas系列产品：提供AI训练、推理卡及训练服务器.</li>
<li>CANN（异构计算架构）：芯片算子库和自动化算法开发工具。</li>
<li>ModelBox：适用于端边云场景的AI推理应用开发框架，提供标准SDK API接口。</li>
<li>MindSpore（AI框架）：支持“端-边-云”独立和协同的统一训练和推理框架。</li>
<li>MindX SDK（昇腾SDK）：行业SDK和应用解决方案。</li>
<li>mxIndex：对于大规模特征检索/聚类的应用场景需求，基于开源Faiss框架，提供极简易用、高性能API。</li>
<li>mxManufacture：提供制造业视觉质检相关API。</li>
<li>mxVision：提供智能视频分析相关API。</li>
<li>ModelArts（AI开发平台）：华为云AI开发平台。</li>
<li>MindStudio（全流程开发工具链）：AI全流程开发IDE。</li>
</ul>
<p>推理方式</p>
<ul>
<li>离线推理：是基于原有AI框架模型转换OM模型，不依赖于AI框架执行推理的场景。</li>
<li>在线推理：是将原有AI框架做推理的应用快速迁移至昇腾AI处理器上，依赖于AI框架执行推理的场景。</li>
</ul>
<h2 id="模型格式"><a href="#模型格式" class="headerlink" title="模型格式"></a>模型格式</h2><h3 id="OM"><a href="#OM" class="headerlink" title="OM"></a>OM</h3><p>OM，全称Offline Model，华为Ascend AI处理器支持的离线模型，实现算子调度的优化，权值数据重排、压缩，内存使用优化等可以脱离设备完成的预处理功能。 TensorRT，NVIDIA 推出的高性能深度学习推理的SDK，包括深度推理优化器和runtime，提高深度学习模型在边缘设备上的推断速度。</p>
<h3 id="ONNX"><a href="#ONNX" class="headerlink" title="ONNX"></a>ONNX</h3><p>ONNX（即开放神经网络交换）是一种用于深度学习模型的开源标准，用来表示深度学习模型的开放格式。所谓开放就是 ONNX 定义了一组与环境、平台均无关的标准格式，来增强各种AI模型的可交互性。是由 Facebook 和 Microsoft 共同开发的，目的是让研究人员和工程师更容易在不同的深度学习框架和硬件平台之间迁移模型。</p>
<p>ONNX 的主要优点之一是它允许轻松地从一个框架（例如 PyTorch）导出模型，并导入到另一个框架（例如 TensorFlow）中。这对于想要尝试不同框架来训练和部署模型的研究人员，或者需要在不同硬件平台上部署模型的工程师特别有吸引力。</p>
<h4 id="查看onnx模型信息"><a href="#查看onnx模型信息" class="headerlink" title="查看onnx模型信息"></a>查看onnx模型信息</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导入并查看模型</span></span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnxruntime</span><br><span class="line">model_name=<span class="string">&quot;./trans/text_encoder/model.onnx&quot;</span></span><br><span class="line">onnx_model=onnx.load(model_name)</span><br><span class="line">onnx.checker.check_model(onnx_model)</span><br><span class="line"><span class="comment"># 查看模型的输入和输出信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Inputs:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span> <span class="keyword">in</span> onnx_model.graph.<span class="built_in">input</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">input</span>.name, <span class="built_in">input</span>.<span class="built_in">type</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Outputs:&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> output <span class="keyword">in</span> onnx_model.graph.output:</span><br><span class="line">    <span class="built_in">print</span>(output.name, output.<span class="built_in">type</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看模型的图结构</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Graph structure:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(onnx_model.graph)</span><br><span class="line"></span><br><span class="line">ort_session=onnxruntime.InferenceSession(model_name)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input_nodes_size=&quot;</span>,<span class="built_in">len</span>(ort_session.get_inputs()))</span><br><span class="line"><span class="keyword">for</span> input_node <span class="keyword">in</span> ort_session.get_inputs():</span><br><span class="line">    <span class="built_in">print</span>(input_node.name)</span><br></pre></td></tr></table></figure>
<h4 id="pytorch运行onnx"><a href="#pytorch运行onnx" class="headerlink" title="pytorch运行onnx"></a>pytorch运行onnx</h4><ol>
<li>导入模型</li>
<li>数据导入并预处理为输入格式</li>
<li>模型运行并输出结果</li>
<li>解析输出结果</li>
</ol>
<p>使用onnx_runtime运行模型<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ort_session=onnxruntime.InferenceSession(model_dir)</span><br><span class="line">input_name = ort_session.get_inputs()[<span class="number">0</span>].name  </span><br><span class="line"><span class="comment"># 获取模型的输入名称，这里假设模型只有一个输入</span></span><br><span class="line">inputs = &#123;input_name: input_data&#125;</span><br><span class="line">output = ort_session.run(<span class="literal">None</span>, inputs)</span><br></pre></td></tr></table></figure></p>
<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><h3 id="AscendCL图片分类应用"><a href="#AscendCL图片分类应用" class="headerlink" title="AscendCL图片分类应用"></a>AscendCL图片分类应用</h3><p>ResNet50 是一种非常流行的深度神经网络模型，它是由微软研究院提出的。ResNet50 是 Residual Network（残差网络）的一个具体实现，采用了深度残差学习的思想。残差学习的核心概念是通过引入”残差块”（residual block），使得网络可以学习到相对于之前层的残差而不是直接学习目标映射。这种设计能够有效地解决深度神经网络训练过程中的梯度消失和梯度爆炸问题，使得可以训练非常深的网络。</p>
<p>ResNet50 具有50层深度，由多个残差块组成，其中包括卷积层、全局平均池化层和全连接层。该模型在 ImageNet 数据集上训练有素，可以用于图像分类和特征提取等任务。由于其卓越的性能和相对较小的模型尺寸，ResNet50 成为了许多计算机视觉任务的首选模型之一。</p>
<p>在实际应用中，ResNet50 被广泛用于图像识别、目标检测、图像细粒度分类等领域。它的设计理念和优秀性能使得它成为了深度学习领域中的经典模型之一。</p>
<p>目录结构<br>├── data         // 用于存放测试图片<br>├── model        // 用于存放模型文件<br>├── scripts       // 用于存放运行样例的脚本<br>├── src          // 用于存放源码</p>
<h4 id="设置环境变量。"><a href="#设置环境变量。" class="headerlink" title="设置环境变量。"></a>设置环境变量。</h4><h5 id="设置CANN依赖的基础环境变量"><a href="#设置CANN依赖的基础环境变量" class="headerlink" title="设置CANN依赖的基础环境变量"></a>设置CANN依赖的基础环境变量</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/local/Ascend/ascend-toolkit/set_env.sh</span><br><span class="line">#如果用户环境存在多个python3版本，则指定使用python3.7.5版本</span><br><span class="line">export PATH=/usr/local/python3.7.5/bin:$PATH</span><br><span class="line">#设置python3.7.5库文件路径</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/python3.7.5/lib:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure>
<h5 id="配置程序编译依赖的头文件与库文件路径"><a href="#配置程序编译依赖的头文件与库文件路径" class="headerlink" title="配置程序编译依赖的头文件与库文件路径"></a>配置程序编译依赖的头文件与库文件路径</h5><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export DDK_PATH=/usr/local/Ascend/ascend-toolkit/latest </span><br><span class="line">export NPU_HOST_LIB=$DDK_PATH/runtime/lib64/stub</span><br></pre></td></tr></table></figure>
<h4 id="安装opencv"><a href="#安装opencv" class="headerlink" title="安装opencv"></a>安装opencv</h4><p>查看opencv包的信息，未查询到有关信息<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pkg-config opencv --libs</span><br></pre></td></tr></table></figure></p>
<p>上传文件到服务器：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp opencv-3.3.1.zip root@172.27.84.235:/home/</span><br><span class="line">NSz6BE17</span><br></pre></td></tr></table></figure></p>
<p>解压<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /home</span><br><span class="line">unzip opencv-3.3.1.zip</span><br></pre></td></tr></table></figure></p>
<p>配置cmake<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br></pre></td></tr></table></figure></p>
<p>编译源文件（很久）安装<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><br>配置环境（每次使用都要配置，除非写到环境管理文件中）<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig/</span><br></pre></td></tr></table></figure><br>查看opencv包的信息<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pkg-config opencv --libs</span><br></pre></td></tr></table></figure></p>
<h4 id="完整环境导入"><a href="#完整环境导入" class="headerlink" title="完整环境导入"></a>完整环境导入</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/opt/ascend-toolkit/set_env.sh</span><br><span class="line">export PATH=/usr/local/python3.7.5/bin:$PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/python3.7.5/lib:$LD_LIBRARY_PATH</span><br><span class="line">export DDK_PATH=/opt/ascend-toolkit/latest</span><br><span class="line">export NPU_HOST_LIB=$DDK_PATH/runtime/lib64/stub</span><br><span class="line">export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/usr/local/lib/pkgconfig/</span><br></pre></td></tr></table></figure>
<h4 id="样例运行"><a href="#样例运行" class="headerlink" title="样例运行"></a>样例运行</h4><p>获取onnx模型并转化为om模型。芯片类型是910B！！！<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /home/samples/inference/modelInference/sampleResnetQuickStart/python/model </span><br><span class="line">wget https://obs-9be7.obs.cn-east-2.myhuaweicloud.com/003_Atc_Models/resnet50/resnet50.onnx </span><br><span class="line">atc --model=resnet50.onnx --framework=5 --output=resnet50 --input_shape=&quot;actual_input_1:1,3,224,224&quot;  --soc_version=Ascend910B</span><br></pre></td></tr></table></figure><br>atc命令中各参数的解释如下，详细约束说明请参见《ATC模型转换指南》。</p>
<ul>
<li>model：ResNet-50网络的模型文件的路径。</li>
<li>framework：原始框架类型。5表示ONNX。</li>
<li>output：resnet50.om模型文件的路径。请注意，记录保存该om模型文件的路径，后续开发应用时需要使用。</li>
<li>input_shape：模型输入数据的shape。</li>
<li>soc_version：昇腾AI处理器的版本。</li>
</ul>
<p>获取样例的测试图片dog1_1024_683.jpg，放在data目录下。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /home/samples/inference/modelInference/sampleResnetQuickStart/python/data </span><br><span class="line">wget https://obs-9be7.obs.cn-east-2.myhuaweicloud.com/models/aclsample/dog1_1024_683.jpg</span><br></pre></td></tr></table></figure></p>
<p>执行以下脚本运行样例：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /home/samples/inference/modelInference/sampleResnetQuickStart/python/scripts </span><br><span class="line">bash sample_run.sh</span><br><span class="line"></span><br><span class="line">[INFO] The sample starts to run</span><br><span class="line">out_dog1_1024_683.jpg</span><br><span class="line">label:162  conf:0.902203  class:beagle</span><br><span class="line">*****run finish******</span><br></pre></td></tr></table></figure><br>成功运行</p>
<h3 id="stable-diffusion-2-1"><a href="#stable-diffusion-2-1" class="headerlink" title="stable-diffusion-2-1"></a>stable-diffusion-2-1</h3><h4 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h4><p>This is a model that can be used to generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H).</p>
<p>服务器连不上hugging face，可以采用 <a target="_blank" rel="noopener" href="https://aliendao.cn/">https://aliendao.cn/</a> 这样的静态镜像网站下载。</p>
<p>ascend910B的出图速度非常的快，经过测试，在推理轮数50轮下，平均一轮的速度为0.5 s/it，在笔记本上的cpu上跑是大概50 s/it，显存占用大概为20GB。</p>
<h3 id="stable-diffusion-xl-base-1-0"><a href="#stable-diffusion-xl-base-1-0" class="headerlink" title="stable-diffusion-xl-base-1.0"></a>stable-diffusion-xl-base-1.0</h3><h4 id="模型介绍-1"><a href="#模型介绍-1" class="headerlink" title="模型介绍"></a>模型介绍</h4><p>SDXL consists of an ensemble of experts pipeline for latent diffusion: In a first step, the base model is used to generate (noisy) latents, which are then further processed with a refinement model (available here: <a target="_blank" rel="noopener" href="https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/">https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/</a>) specialized for the final denoising steps.</p>
<h4 id="模型使用"><a href="#模型使用" class="headerlink" title="模型使用"></a>模型使用</h4><p>使用这个模型出现了很多问题，首先是下载的时候，出现了很多次断联，然后重复下载了很多次，模型导入的时候，也有不完整的报错。后来运行的时候，没有任何警告或者报错，但是输出的图片是彩色的乱码，搜索了很久相关信息，还是没找到解决方法。</p>
<p>猜测可能是由于以下原因之一：</p>
<ul>
<li>镜像网站上提供的vae模型版本不对</li>
<li>镜像网站上下载的模型文件不完整</li>
<li>ascend对pytorch的适配出现问题</li>
</ul>
<h3 id="chatglm2-6b"><a href="#chatglm2-6b" class="headerlink" title="chatglm2-6b"></a>chatglm2-6b</h3><h4 id="模型介绍-2"><a href="#模型介绍-2" class="headerlink" title="模型介绍"></a>模型介绍</h4><p>ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。</p>
<h4 id="模型使用-1"><a href="#模型使用-1" class="headerlink" title="模型使用"></a>模型使用</h4><p>部署完成后，测试发现，在atlas平台上输出的速度非常慢，平均几秒甚至几十秒一个token。有一个warning，说使用了代价很高的算子，可能是算子不太适配，在转化的过程中，转化过程比较低效。不过效果确实还不错。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://jayerine.top">Jay and Erine</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://jayerine.top/2024/02/02/92e6307b5e88/">https://jayerine.top/2024/02/02/92e6307b5e88/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%8D%8E%E4%B8%BA/">华为</a></div><div class="post_share"><div class="social-share" data-image="https://s1.ax1x.com/2023/03/02/ppFkiKx.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/02/06/b7116e56a5fc/" title="计算机组成原理 第一章"><img class="cover" src="https://s1.ax1x.com/2023/03/05/ppE2Y0U.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">计算机组成原理 第一章</div></div></a></div><div class="next-post pull-right"><a href="/2021/10/25/d44479f49fde/" title="《月亮与六便士》读后感"><img class="cover" src="https://s1.ax1x.com/2023/03/03/ppA3ZxP.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next</div><div class="next_info">《月亮与六便士》读后感</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/02/06/125b95e4ccef/" title="鲲鹏软件迁移"><img class="cover" src="https://s1.ax1x.com/2023/03/01/ppiUGb8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-06</div><div class="title">鲲鹏软件迁移</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s1.ax1x.com/2023/02/27/ppCAPEQ.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Jay and Erine</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">36</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">18</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/JayLXL/BlogDevelopment"><i class="fab fa-github"></i><span>Github</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Jay and Erine are best friend.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-number">1.</span> <span class="toc-text">环境</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BF%A1%E6%81%AF"><span class="toc-number">1.1.</span> <span class="toc-text">服务器信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%91%BD%E4%BB%A4"><span class="toc-number">1.2.</span> <span class="toc-text">Linux服务器命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9B%AE%E5%BD%95"><span class="toc-number">1.3.</span> <span class="toc-text">常用Linux服务器目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#atlas%E8%AE%BE%E5%A4%87"><span class="toc-number">1.4.</span> <span class="toc-text">atlas设备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AIPP%EF%BC%88AI-Preprocessing-Pipeline%EF%BC%89%EF%BC%9A"><span class="toc-number">1.5.</span> <span class="toc-text">AIPP（AI Preprocessing Pipeline）：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DVPP%EF%BC%88Data-Visuo-Processing-Pipeline%EF%BC%89%EF%BC%9A"><span class="toc-number">1.5.1.</span> <span class="toc-text">DVPP（Data Visuo-Processing Pipeline）：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#conda"><span class="toc-number">1.6.</span> <span class="toc-text">conda</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84conda%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4"><span class="toc-number">1.6.1.</span> <span class="toc-text">常用的conda环境管理命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">1.6.2.</span> <span class="toc-text">注意事项</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mindspore"><span class="toc-number">1.7.</span> <span class="toc-text">mindspore</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pytorch"><span class="toc-number">1.8.</span> <span class="toc-text">pytorch</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#onnx%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="toc-number">2.</span> <span class="toc-text">onnx模型使用流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7"><span class="toc-number">3.</span> <span class="toc-text">开发工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%A0%BC%E5%BC%8F"><span class="toc-number">4.</span> <span class="toc-text">模型格式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#OM"><span class="toc-number">4.1.</span> <span class="toc-text">OM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ONNX"><span class="toc-number">4.2.</span> <span class="toc-text">ONNX</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8Bonnx%E6%A8%A1%E5%9E%8B%E4%BF%A1%E6%81%AF"><span class="toc-number">4.2.1.</span> <span class="toc-text">查看onnx模型信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pytorch%E8%BF%90%E8%A1%8Connx"><span class="toc-number">4.2.2.</span> <span class="toc-text">pytorch运行onnx</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">部署应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AscendCL%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E5%BA%94%E7%94%A8"><span class="toc-number">5.1.</span> <span class="toc-text">AscendCL图片分类应用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E3%80%82"><span class="toc-number">5.1.1.</span> <span class="toc-text">设置环境变量。</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AECANN%E4%BE%9D%E8%B5%96%E7%9A%84%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">5.1.1.1.</span> <span class="toc-text">设置CANN依赖的基础环境变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E7%A8%8B%E5%BA%8F%E7%BC%96%E8%AF%91%E4%BE%9D%E8%B5%96%E7%9A%84%E5%A4%B4%E6%96%87%E4%BB%B6%E4%B8%8E%E5%BA%93%E6%96%87%E4%BB%B6%E8%B7%AF%E5%BE%84"><span class="toc-number">5.1.1.2.</span> <span class="toc-text">配置程序编译依赖的头文件与库文件路径</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%89%E8%A3%85opencv"><span class="toc-number">5.1.2.</span> <span class="toc-text">安装opencv</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E7%8E%AF%E5%A2%83%E5%AF%BC%E5%85%A5"><span class="toc-number">5.1.3.</span> <span class="toc-text">完整环境导入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B7%E4%BE%8B%E8%BF%90%E8%A1%8C"><span class="toc-number">5.1.4.</span> <span class="toc-text">样例运行</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stable-diffusion-2-1"><span class="toc-number">5.2.</span> <span class="toc-text">stable-diffusion-2-1</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">5.2.1.</span> <span class="toc-text">模型介绍</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stable-diffusion-xl-base-1-0"><span class="toc-number">5.3.</span> <span class="toc-text">stable-diffusion-xl-base-1.0</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D-1"><span class="toc-number">5.3.1.</span> <span class="toc-text">模型介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="toc-number">5.3.2.</span> <span class="toc-text">模型使用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#chatglm2-6b"><span class="toc-number">5.4.</span> <span class="toc-text">chatglm2-6b</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D-2"><span class="toc-number">5.4.1.</span> <span class="toc-text">模型介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8-1"><span class="toc-number">5.4.2.</span> <span class="toc-text">模型使用</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/02/06/137184bfb43a/" title="面试记录"><img src="https://blog-pics.obs.cn-north-4.myhuaweicloud.com/%E5%B0%81%E9%9D%A2/Jay%E7%AE%80%E5%8E%86%E5%B0%81%E9%9D%A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="面试记录"/></a><div class="content"><a class="title" href="/2024/02/06/137184bfb43a/" title="面试记录">面试记录</a><time datetime="2024-02-06T04:00:21.577Z" title="Created 2024-02-06 12:00:21">2024-02-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/06/bb4263303c4c/" title="git使用"><img src="https://s1.ax1x.com/2023/03/10/ppnbMQ0.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="git使用"/></a><div class="content"><a class="title" href="/2024/02/06/bb4263303c4c/" title="git使用">git使用</a><time datetime="2024-02-06T04:00:21.577Z" title="Created 2024-02-06 12:00:21">2024-02-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/06/0e751f91670a/" title="两棵树（可持久线段树+Hash+高精度数）"><img src="https://s1.ax1x.com/2023/03/02/ppFFvaF.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="两棵树（可持久线段树+Hash+高精度数）"/></a><div class="content"><a class="title" href="/2024/02/06/0e751f91670a/" title="两棵树（可持久线段树+Hash+高精度数）">两棵树（可持久线段树+Hash+高精度数）</a><time datetime="2024-02-06T04:00:21.577Z" title="Created 2024-02-06 12:00:21">2024-02-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/06/f554b43ffd79/" title="vscode配置opencv C++ 环境与人脸检测（不是人脸识别）算法的尝试"><img src="https://s1.ax1x.com/2023/03/02/ppFFjVU.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="vscode配置opencv C++ 环境与人脸检测（不是人脸识别）算法的尝试"/></a><div class="content"><a class="title" href="/2024/02/06/f554b43ffd79/" title="vscode配置opencv C++ 环境与人脸检测（不是人脸识别）算法的尝试">vscode配置opencv C++ 环境与人脸检测（不是人脸识别）算法的尝试</a><time datetime="2024-02-06T04:00:21.577Z" title="Created 2024-02-06 12:00:21">2024-02-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/06/feae35703595/" title="奇怪名词解释"><img src="https://blog-pics.obs.cn-north-4.myhuaweicloud.com/%E5%B0%81%E9%9D%A2/%E5%A5%87%E6%80%AA%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E5%B0%81%E9%9D%A2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="奇怪名词解释"/></a><div class="content"><a class="title" href="/2024/02/06/feae35703595/" title="奇怪名词解释">奇怪名词解释</a><time datetime="2024-02-06T04:00:21.577Z" title="Created 2024-02-06 12:00:21">2024-02-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Jay and Erine</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script><script src="https://gcore.jsdelivr.net/gh/xiabo2/CDN@latest/fishes.js"></script><script type="text/javascript" src="/js/fairyDustCursor.js"></script><script id="canvas_nest" defer="defer" color="255,240,245" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>